{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8152fc-8f93-41e4-aa74-4984c5cf54fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 13:07:13.675041: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entra√Ænement: Charg√© 10000 images (indices 0 √† 9999)\n",
      "   Distribution des classes: [7366  721 1475  247  191]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Paths\n",
    "DIRECTORY = \"/home/user/datasets/resized_train_cropped/resized_train_cropped\"\n",
    "LABELS_FILE = \"/home/user/datasets/trainLabels_cropped.csv\"\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_dict = dict(zip(labels_df['image'], labels_df['level']))\n",
    "\n",
    "# Collect all images in all subfolders\n",
    "all_files = glob.glob(os.path.join(DIRECTORY, \"**\", \"*.jpeg\"), recursive=True)\n",
    "\n",
    "# ===== PARAM√àTRES FIXES POUR L'ENTRA√éNEMENT =====\n",
    "START_INDEX = 0      # Toujours commencer √† 0\n",
    "BATCH_SIZE = 10000    # Toujours charger 1000 images\n",
    "# ===============================================\n",
    "\n",
    "images_train = []\n",
    "labels_train = []\n",
    "current_index = 0\n",
    "loaded_count = 0\n",
    "\n",
    "for filepath in all_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # Charger seulement BATCH_SIZE images\n",
    "    if loaded_count >= BATCH_SIZE:\n",
    "        break\n",
    "\n",
    "    img = load_img(filepath, target_size=(256, 256))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "\n",
    "    # Match labels\n",
    "    base_name = re.sub(r'\\s*\\(.*\\)', '', filename.split('.')[0])\n",
    "    label = labels_dict.get(base_name)\n",
    "\n",
    "    if label is not None:\n",
    "        images_train.append(img_array)\n",
    "        labels_train.append(label)\n",
    "        loaded_count += 1\n",
    "        current_index += 1\n",
    "    else:\n",
    "        current_index += 1\n",
    "\n",
    "# Convert to arrays\n",
    "images_train = np.array(images_train)\n",
    "labels_train_raw = np.array(labels_train)\n",
    "\n",
    "# One-hot encoding\n",
    "labels_train = to_categorical(labels_train, num_classes=5)\n",
    "\n",
    "print(f\"‚úÖ Entra√Ænement: Charg√© {images_train.shape[0]} images (indices 0 √† {loaded_count - 1})\")\n",
    "print(f\"   Distribution des classes: {np.bincount(labels_train_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0ae432-c5c1-4ef0-8a26-a9b91d48cfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test: Charg√© 1 images (indices 20000 √† 20000)\n",
      "   Classes: [0]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Paths\n",
    "DIRECTORY = \"/home/user/datasets/resized_train_cropped/resized_train_cropped\"\n",
    "LABELS_FILE = \"/home/user/datasets/trainLabels_cropped.csv\"\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_dict = dict(zip(labels_df['image'], labels_df['level']))\n",
    "\n",
    "# Collect all images in all subfolders\n",
    "all_files = glob.glob(os.path.join(DIRECTORY, \"**\", \"*.jpeg\"), recursive=True)\n",
    "\n",
    "# ===== PARAM√àTRES VARIABLES POUR LE TEST =====\n",
    "START_INDEX = 20000   # üîß MODIFIEZ ICI: o√π commencer (ex: 1000, 2000, etc.)\n",
    "BATCH_SIZE = 1       # üîß MODIFIEZ ICI: combien d'images (ex: 1, 5, 10, etc.)\n",
    "# ============================================\n",
    "\n",
    "images_test = []\n",
    "labels_test = []\n",
    "current_index = 0\n",
    "loaded_count = 0\n",
    "\n",
    "for filepath in all_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # Skip jusqu'√† START_INDEX\n",
    "    if current_index < START_INDEX:\n",
    "        base_name = re.sub(r'\\s*\\(.*\\)', '', filename.split('.')[0])\n",
    "        if labels_dict.get(base_name) is not None:\n",
    "            current_index += 1\n",
    "        continue\n",
    "\n",
    "    # Charger seulement BATCH_SIZE images\n",
    "    if loaded_count >= BATCH_SIZE:\n",
    "        break\n",
    "\n",
    "    img = load_img(filepath, target_size=(256, 256))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "\n",
    "    # Match labels\n",
    "    base_name = re.sub(r'\\s*\\(.*\\)', '', filename.split('.')[0])\n",
    "    label = labels_dict.get(base_name)\n",
    "\n",
    "    if label is not None:\n",
    "        images_test.append(img_array)\n",
    "        labels_test.append(label)\n",
    "        loaded_count += 1\n",
    "        current_index += 1\n",
    "    else:\n",
    "        current_index += 1\n",
    "\n",
    "# Convert to arrays\n",
    "images_test = np.array(images_test)\n",
    "labels_test_raw = np.array(labels_test)\n",
    "\n",
    "# One-hot encoding\n",
    "labels_test = to_categorical(labels_test, num_classes=5)\n",
    "\n",
    "print(f\"‚úÖ Test: Charg√© {images_test.shape[0]} images (indices {START_INDEX} √† {START_INDEX + loaded_count - 1})\")\n",
    "print(f\"   Classes: {labels_test_raw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e77994-7ac6-428c-b858-2017f09a5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = images_train.reshape((images_train.shape[0], -1))  # Shape: (n_train, 256*256*3)\n",
    "y_train = np.argmax(labels_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c4a045-622d-45fc-8b50-210db76ee5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = images_test.reshape((images_test.shape[0], -1))  # Shape: (n_train, 256*256*3)\n",
    "y_test = np.argmax(labels_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2d37ee9-f028-48ea-9f99-8cb9ee66d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class knn:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fitt(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predire(self, test_sample):\n",
    "        #test_sample = np.array(test_sample).ravel()\n",
    "        # Vectorized Euclidean distance: sqrt(sum((x_train - test_sample)^2))\n",
    "        distances = np.sqrt(np.sum((self.x_train - test_sample)**2, axis=1))\n",
    "        print(f'thread {threading.get_ident()} calcule {len(distances)} distances')\n",
    "        # Get indices of k smallest distances\n",
    "        k_indices = np.argpartition(distances, self.k)[:self.k]\n",
    "\n",
    "        # Get the labels of those k neighbors\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "\n",
    "        # Return the most common label\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c06adfb-3d08-44a3-8a1e-462c96a5e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_seq = knn(k=5)\n",
    "knn_seq.fitt(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "587acbca-f08f-4274-8fb7-dd7fec7f660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 128909034173504 calcule 10000 distances\n",
      "Temps de pr√©diction = 10.4554 secondes\n",
      "Predicted labels: [np.int64(0)]\n",
      "True labels: [0]\n",
      "Correct? ‚úÖ\n",
      "\n",
      "D√©tails:\n",
      "  Image 0: Pr√©diction=0, Vrai=0 ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "predictions = []\n",
    "debut = time.perf_counter()\n",
    "for i in x_test:\n",
    "  prediction = knn_seq.predire(i)\n",
    "  predictions.append(prediction)\n",
    "fin = time.perf_counter()\n",
    "print(f'Temps de pr√©diction = {fin-debut:.4f} secondes')\n",
    "print(f\"Predicted labels: {predictions}\")\n",
    "print(f\"True labels: {labels_test_raw}\")  # ‚úÖ Correction: labels_test_raw au lieu de y_test\n",
    "\n",
    "# ‚úÖ Correction: comparaison correcte\n",
    "correct = all(pred == true for pred, true in zip(predictions, labels_test_raw))\n",
    "print(f\"Correct? {'‚úÖ' if correct else '‚ùå'}\")\n",
    "\n",
    "# Afficher les d√©tails par image\n",
    "print(\"\\nD√©tails:\")\n",
    "for idx, (pred, true) in enumerate(zip(predictions, labels_test_raw)):\n",
    "    match = \"‚úÖ\" if pred == true else \"‚ùå\"\n",
    "    print(f\"  Image {idx}: Pr√©diction={pred}, Vrai={true} {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eeef96f-5df8-4d94-a686-13dde91a66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from multiprocessing import Pool, Array\n",
    "import ctypes\n",
    "\n",
    "# Global shared arrays (accessed by workers without copying)\n",
    "shared_x_base = None\n",
    "shared_y_base = None\n",
    "x_shape = None\n",
    "x_dtype = None\n",
    "\n",
    "def init_worker(x_base, y_base, shape, dtype):\n",
    "    \"\"\"Initialize worker process with shared memory access.\"\"\"\n",
    "    global shared_x_base, shared_y_base, x_shape, x_dtype\n",
    "    shared_x_base = x_base\n",
    "    shared_y_base = y_base\n",
    "    x_shape = shape\n",
    "    x_dtype = dtype\n",
    "\n",
    "def worker_distances(args):\n",
    "    \"\"\"Calculate distances for a chunk - returns top k.\"\"\"\n",
    "    start, end, test_sample, k = args\n",
    "    \n",
    "    # Reconstruct numpy array from shared memory\n",
    "    x_np = np.frombuffer(shared_x_base, dtype=x_dtype).reshape(x_shape)\n",
    "    y_np = np.frombuffer(shared_y_base, dtype=np.int32)\n",
    "    \n",
    "    # Slice our chunk\n",
    "    x_chunk = x_np[start:end]\n",
    "    y_chunk = y_np[start:end]\n",
    "    \n",
    "    # Compute squared distances (skip sqrt for speed)\n",
    "    distances = np.sum((x_chunk - test_sample)**2, axis=1)\n",
    "    \n",
    "    # Get k smallest from this chunk\n",
    "    k_local = min(k, len(distances))\n",
    "    k_idx = np.argpartition(distances, k_local-1)[:k_local]\n",
    "    \n",
    "    # Return distances and labels as arrays\n",
    "    return distances[k_idx], y_chunk[k_idx]\n",
    "\n",
    "class ParallelKNN:\n",
    "    def __init__(self, k, n_workers=None):\n",
    "        self.k = k\n",
    "        self.n_workers = n_workers or 4\n",
    "        self.pool = None\n",
    "        \n",
    "    def fitt(self, x_train, y_train):\n",
    "        \"\"\"Fit model - creates persistent pool with shared memory.\"\"\"\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.n_samples, self.n_features = x_train.shape\n",
    "        \n",
    "        # Create shared memory arrays\n",
    "        x_flat = x_train.flatten()\n",
    "        self.shared_x = Array(ctypes.c_double, x_flat, lock=False)\n",
    "        self.shared_y = Array(ctypes.c_int32, y_train, lock=False)\n",
    "        \n",
    "        # Create persistent pool\n",
    "        self.pool = Pool(\n",
    "            processes=self.n_workers,\n",
    "            initializer=init_worker,\n",
    "            initargs=(self.shared_x, self.shared_y, \n",
    "                     x_train.shape, x_train.dtype)\n",
    "        )\n",
    "        \n",
    "        # Pre-calculate chunk boundaries\n",
    "        chunk_size = (self.n_samples + self.n_workers - 1) // self.n_workers\n",
    "        self.chunks = []\n",
    "        for i in range(self.n_workers):\n",
    "            start = i * chunk_size\n",
    "            end = min(start + chunk_size, self.n_samples)\n",
    "            if start < self.n_samples:\n",
    "                self.chunks.append((start, end))\n",
    "    \n",
    "    def predire(self, test_sample):\n",
    "        \"\"\"Predict using parallel distance computation.\"\"\"\n",
    "        if self.pool is None:\n",
    "            raise RuntimeError(\"Call fitt() first\")\n",
    "        \n",
    "        # Prepare tasks (minimal data transfer)\n",
    "        tasks = [(start, end, test_sample, self.k) for start, end in self.chunks]\n",
    "        \n",
    "        # Parallel computation\n",
    "        results = self.pool.map(worker_distances, tasks)\n",
    "        \n",
    "        # Merge k*workers nearest neighbors\n",
    "        all_dists = np.concatenate([r[0] for r in results])\n",
    "        all_labels = np.concatenate([r[1] for r in results])\n",
    "        \n",
    "        # Final k nearest\n",
    "        final_k = min(self.k, len(all_dists))\n",
    "        final_idx = np.argpartition(all_dists, final_k-1)[:final_k]\n",
    "        final_labels = all_labels[final_idx]\n",
    "        \n",
    "        # Most common label\n",
    "        return Counter(final_labels).most_common(1)[0][0]\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close pool when done.\"\"\"\n",
    "        if self.pool:\n",
    "            self.pool.close()\n",
    "            self.pool.join()\n",
    "            self.pool = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85337b48-76b9-415f-8f3c-f0685f834dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "knn_mult = ParallelKNN(k=5, n_workers=4)\n",
    "knn_mult.fitt(x_train, y_train)\n",
    "\n",
    "debut = time.time()\n",
    "prediction = knn_mult.predire(x_test)\n",
    "fin = time.time()\n",
    "print(f\"Predicted class: {prediction == y_test}\")\n",
    "print(f'prediction time: {fin-debut:.2f} secondes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377ff7c-320a-4bb9-b232-c3e3f229021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'prediction time: {fin-debut:.2f} secondes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92f63e54-726f-44dc-9f38-6f2d2072b59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement parsl.channels (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[31mERROR: No matching distribution found for parsl.channels\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install parsl.channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e471ff-4eb0-4147-abb9-6cf4222e0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.executors import ThreadPoolExecutor\n",
    "\n",
    "from parsl.providers import LocalProvider\n",
    "\n",
    "@python_app\n",
    "def calculate_distances_chunk(x_chunk, y_chunk, test_sample, k):\n",
    "    \"\"\"\n",
    "    Parsl app: Calculate distances for a data chunk.\n",
    "    Returns k nearest distances and labels from this chunk.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate squared Euclidean distances (skip sqrt for efficiency)\n",
    "    distances = np.sum((x_chunk - test_sample)**2, axis=1)\n",
    "    \n",
    "    # Get k smallest distances from this chunk\n",
    "    chunk_k = min(k, len(distances))\n",
    "    k_indices = np.argpartition(distances, chunk_k - 1)[:chunk_k]\n",
    "    \n",
    "    # Return distances and corresponding labels\n",
    "    return distances[k_indices], y_chunk[k_indices]\n",
    "\n",
    "\n",
    "class ParslKNN:\n",
    "    def __init__(self, k, n_workers=4):\n",
    "        \"\"\"\n",
    "        Initialize Parsl KNN classifier.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of nearest neighbors\n",
    "            n_workers: Number of parallel workers\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.n_workers = n_workers\n",
    "        self.parsl_loaded = False\n",
    "        \n",
    "\n",
    "    def fitt(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the KNN model with training data.\n",
    "        \n",
    "        Args:\n",
    "            x_train: Training features (n_samples, n_features)\n",
    "            y_train: Training labels (n_samples,)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.x_train = np.asarray(x_train)\n",
    "        self.y_train = np.asarray(y_train)\n",
    "        self.n_samples = len(self.x_train)\n",
    "        \n",
    "        # Pre-split data into chunks for workers\n",
    "        chunk_size = (self.n_samples + self.n_workers - 1) // self.n_workers\n",
    "        self.data_chunks = []\n",
    "        \n",
    "        for i in range(self.n_workers):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, self.n_samples)\n",
    "            \n",
    "            if start_idx < self.n_samples:\n",
    "                self.data_chunks.append((\n",
    "                    self.x_train[start_idx:end_idx],\n",
    "                    self.y_train[start_idx:end_idx]\n",
    "                ))\n",
    "        \n",
    "        print(f\"Data split into {len(self.data_chunks)} chunks for {self.n_workers} workers\")\n",
    "    \n",
    "    def predire(self, test_sample):\n",
    "        \"\"\"\n",
    "        Predict the class label for a test sample using parallel computation.\n",
    "        \n",
    "        Args:\n",
    "            test_sample: Test sample features (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class label\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'data_chunks'):\n",
    "            raise RuntimeError(\"Model not fitted. Call fitt() first.\")\n",
    "        \n",
    "        test_sample = np.asarray(test_sample)\n",
    "        \n",
    "        # Submit parallel tasks to compute distances for each chunk\n",
    "        futures = []\n",
    "        for x_chunk, y_chunk in self.data_chunks:\n",
    "            future = calculate_distances_chunk(x_chunk, y_chunk, test_sample, self.k)\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Wait for all workers to complete and collect results\n",
    "        chunk_results = [future.result() for future in futures]\n",
    "        \n",
    "        # Merge all k*workers nearest neighbors\n",
    "        all_distances = np.concatenate([result[0] for result in chunk_results])\n",
    "        all_labels = np.concatenate([result[1] for result in chunk_results])\n",
    "        \n",
    "        # Select final k nearest neighbors from merged results\n",
    "        final_k = min(self.k, len(all_distances))\n",
    "        final_k_indices = np.argpartition(all_distances, final_k - 1)[:final_k]\n",
    "        final_k_labels = all_labels[final_k_indices]\n",
    "        \n",
    "        # Return most common label (majority vote)\n",
    "        most_common = Counter(final_k_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def predire_batch(self, test_samples):\n",
    "        \"\"\"\n",
    "        Predict labels for multiple test samples.\n",
    "        \n",
    "        Args:\n",
    "            test_samples: Array of test samples (n_test_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            List of predicted labels\n",
    "        \"\"\"\n",
    "        return [self.predire(sample) for sample in test_samples]\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up Parsl resources.\"\"\"\n",
    "        if self.parsl_loaded:\n",
    "            parsl.dfk().cleanup()\n",
    "            parsl.clear()\n",
    "            self.parsl_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89aed10a-5d7b-4761-a765-f64f3cdc8192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7589ea11bfb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(\n",
    "                executors=[\n",
    "                    ThreadPoolExecutor(\n",
    "                        max_threads=12,\n",
    "                        label='knn_workers'\n",
    "                    )\n",
    "                ],\n",
    "                strategy='none',  # Disable auto-scaling for predictable performance\n",
    "            )\n",
    "parsl.load(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e4b7a56-98d7-4d46-925f-40be244c953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into 12 chunks for 12 workers\n",
      "Predicted class: [ True]\n",
      "prediction time: 2.04 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "knn_parsl = ParslKNN(k=5, n_workers=12)\n",
    "knn_parsl.fitt(x_train, y_train)\n",
    "\n",
    "debut = time.time()\n",
    "prediction = knn_parsl.predire(x_test)\n",
    "fin = time.time()\n",
    "print(f\"Predicted class: {prediction == y_test}\")\n",
    "print(f'prediction time: {fin-debut:.2f} secondes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d9e6e59-8aa7-459e-89f9-4a69e4755f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsl.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8cd2ff-cd38-4ea7-bf9e-8e50a2897e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
