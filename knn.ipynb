{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe8152fc-8f93-41e4-aa74-4984c5cf54fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 11:55:28.038422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Entra√Ænement: Charg√© 10000 images (indices 0 √† 9999)\n",
      "   Distribution des classes: [7366  721 1475  247  191]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Paths\n",
    "DIRECTORY = \"/home/user/datasets/resized_train_cropped/resized_train_cropped\"\n",
    "LABELS_FILE = \"/home/user/datasets/trainLabels_cropped.csv\"\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_dict = dict(zip(labels_df['image'], labels_df['level']))\n",
    "\n",
    "# Collect all images in all subfolders\n",
    "all_files = glob.glob(os.path.join(DIRECTORY, \"**\", \"*.jpeg\"), recursive=True)\n",
    "\n",
    "# ===== PARAM√àTRES FIXES POUR L'ENTRA√éNEMENT =====\n",
    "START_INDEX = 0      # Toujours commencer √† 0\n",
    "BATCH_SIZE = 10000    # Toujours charger 1000 images\n",
    "# ===============================================\n",
    "\n",
    "images_train = []\n",
    "labels_train = []\n",
    "current_index = 0\n",
    "loaded_count = 0\n",
    "\n",
    "for filepath in all_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # Charger seulement BATCH_SIZE images\n",
    "    if loaded_count >= BATCH_SIZE:\n",
    "        break\n",
    "\n",
    "    img = load_img(filepath, target_size=(256, 256))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "\n",
    "    # Match labels\n",
    "    base_name = re.sub(r'\\s*\\(.*\\)', '', filename.split('.')[0])\n",
    "    label = labels_dict.get(base_name)\n",
    "\n",
    "    if label is not None:\n",
    "        images_train.append(img_array)\n",
    "        labels_train.append(label)\n",
    "        loaded_count += 1\n",
    "        current_index += 1\n",
    "    else:\n",
    "        current_index += 1\n",
    "\n",
    "# Convert to arrays\n",
    "images_train = np.array(images_train)\n",
    "labels_train_raw = np.array(labels_train)\n",
    "\n",
    "# One-hot encoding\n",
    "labels_train = to_categorical(labels_train, num_classes=5)\n",
    "\n",
    "print(f\"‚úÖ Entra√Ænement: Charg√© {images_train.shape[0]} images (indices 0 √† {loaded_count - 1})\")\n",
    "print(f\"   Distribution des classes: {np.bincount(labels_train_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0ae432-c5c1-4ef0-8a26-a9b91d48cfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test: Charg√© 1 images (indices 20000 √† 20000)\n",
      "   Classes: [0]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Paths\n",
    "DIRECTORY = \"/home/user/datasets/resized_train_cropped/resized_train_cropped\"\n",
    "LABELS_FILE = \"/home/user/datasets/trainLabels_cropped.csv\"\n",
    "\n",
    "# Load labels\n",
    "labels_df = pd.read_csv(LABELS_FILE)\n",
    "labels_dict = dict(zip(labels_df['image'], labels_df['level']))\n",
    "\n",
    "# Collect all images in all subfolders\n",
    "all_files = glob.glob(os.path.join(DIRECTORY, \"**\", \"*.jpeg\"), recursive=True)\n",
    "\n",
    "# ===== PARAM√àTRES VARIABLES POUR LE TEST =====\n",
    "START_INDEX = 20000   # üîß MODIFIEZ ICI: o√π commencer (ex: 1000, 2000, etc.)\n",
    "BATCH_SIZE = 1       # üîß MODIFIEZ ICI: combien d'images (ex: 1, 5, 10, etc.)\n",
    "# ============================================\n",
    "\n",
    "images_test = []\n",
    "labels_test = []\n",
    "current_index = 0\n",
    "loaded_count = 0\n",
    "\n",
    "for filepath in all_files:\n",
    "    filename = os.path.basename(filepath)\n",
    "\n",
    "    # Skip jusqu'√† START_INDEX\n",
    "    if current_index < START_INDEX:\n",
    "        base_name = re.sub(r'\\s*\\(.*\\)', '', filename.split('.')[0])\n",
    "        if labels_dict.get(base_name) is not None:\n",
    "            current_index += 1\n",
    "        continue\n",
    "\n",
    "    # Charger seulement BATCH_SIZE images\n",
    "    if loaded_count >= BATCH_SIZE:\n",
    "        break\n",
    "\n",
    "    img = load_img(filepath, target_size=(256, 256))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "\n",
    "    # Match labels\n",
    "    base_name = re.sub(r'\\s*\\(.*\\)', '', filename.split('.')[0])\n",
    "    label = labels_dict.get(base_name)\n",
    "\n",
    "    if label is not None:\n",
    "        images_test.append(img_array)\n",
    "        labels_test.append(label)\n",
    "        loaded_count += 1\n",
    "        current_index += 1\n",
    "    else:\n",
    "        current_index += 1\n",
    "\n",
    "# Convert to arrays\n",
    "images_test = np.array(images_test)\n",
    "labels_test_raw = np.array(labels_test)\n",
    "\n",
    "# One-hot encoding\n",
    "labels_test = to_categorical(labels_test, num_classes=5)\n",
    "\n",
    "print(f\"‚úÖ Test: Charg√© {images_test.shape[0]} images (indices {START_INDEX} √† {START_INDEX + loaded_count - 1})\")\n",
    "print(f\"   Classes: {labels_test_raw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e77994-7ac6-428c-b858-2017f09a5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = images_train.reshape((images_train.shape[0], -1))  # Shape: (n_train, 256*256*3)\n",
    "y_train = np.argmax(labels_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c4a045-622d-45fc-8b50-210db76ee5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = images_test.reshape((images_test.shape[0], -1))  # Shape: (n_train, 256*256*3)\n",
    "y_test = np.argmax(labels_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d37ee9-f028-48ea-9f99-8cb9ee66d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class knn:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "\n",
    "    def fitt(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predire(self, test_sample):\n",
    "        #test_sample = np.array(test_sample).ravel()\n",
    "        # Vectorized Euclidean distance: sqrt(sum((x_train - test_sample)^2))\n",
    "        distances = np.sqrt(np.sum((self.x_train - test_sample)**2, axis=1))\n",
    "        print(f'thread {threading.get_ident()} calcule {len(distances)} distances')\n",
    "        # Get indices of k smallest distances\n",
    "        k_indices = np.argpartition(distances, self.k)[:self.k]\n",
    "\n",
    "        # Get the labels of those k neighbors\n",
    "        k_nearest_labels = self.y_train[k_indices]\n",
    "\n",
    "        # Return the most common label\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c06adfb-3d08-44a3-8a1e-462c96a5e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_seq = knn(k=5)\n",
    "knn_seq.fitt(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "587acbca-f08f-4274-8fb7-dd7fec7f660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread 126979215381568 calcule 10000 distances\n",
      "Temps de pr√©diction = 10.7195 secondes\n",
      "Predicted labels: [np.int64(0)]\n",
      "True labels: [0]\n",
      "Correct? ‚úÖ\n",
      "\n",
      "D√©tails:\n",
      "  Image 0: Pr√©diction=0, Vrai=0 ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "predictions = []\n",
    "debut = time.perf_counter()\n",
    "for i in x_test:\n",
    "  prediction = knn_seq.predire(i)\n",
    "  predictions.append(prediction)\n",
    "fin = time.perf_counter()\n",
    "print(f'Temps de pr√©diction = {fin-debut:.4f} secondes')\n",
    "print(f\"Predicted labels: {predictions}\")\n",
    "print(f\"True labels: {labels_test_raw}\")  # ‚úÖ Correction: labels_test_raw au lieu de y_test\n",
    "\n",
    "# ‚úÖ Correction: comparaison correcte\n",
    "correct = all(pred == true for pred, true in zip(predictions, labels_test_raw))\n",
    "print(f\"Correct? {'‚úÖ' if correct else '‚ùå'}\")\n",
    "\n",
    "# Afficher les d√©tails par image\n",
    "print(\"\\nD√©tails:\")\n",
    "for idx, (pred, true) in enumerate(zip(predictions, labels_test_raw)):\n",
    "    match = \"‚úÖ\" if pred == true else \"‚ùå\"\n",
    "    print(f\"  Image {idx}: Pr√©diction={pred}, Vrai={true} {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1e471ff-4eb0-4147-abb9-6cf4222e0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import parsl\n",
    "from parsl import python_app\n",
    "from parsl.config import Config\n",
    "from parsl.executors import ThreadPoolExecutor\n",
    "\n",
    "from parsl.providers import LocalProvider\n",
    "\n",
    "@python_app\n",
    "def calculate_distances_chunk(x_chunk, y_chunk, test_sample, k):\n",
    "    \"\"\"\n",
    "    Parsl app: Calculate distances for a data chunk.\n",
    "    Returns k nearest distances and labels from this chunk.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate squared Euclidean distances (skip sqrt for efficiency)\n",
    "    distances = np.sum((x_chunk - test_sample)**2, axis=1)\n",
    "    \n",
    "    # Get k smallest distances from this chunk\n",
    "    chunk_k = min(k, len(distances))\n",
    "    k_indices = np.argpartition(distances, chunk_k - 1)[:chunk_k]\n",
    "    \n",
    "    # Return distances and corresponding labels\n",
    "    return distances[k_indices], y_chunk[k_indices]\n",
    "\n",
    "\n",
    "class ParslKNN:\n",
    "    def __init__(self, k, n_workers=4):\n",
    "        \"\"\"\n",
    "        Initialize Parsl KNN classifier.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of nearest neighbors\n",
    "            n_workers: Number of parallel workers\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.n_workers = n_workers\n",
    "        self.parsl_loaded = False\n",
    "        \n",
    "\n",
    "    def fitt(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the KNN model with training data.\n",
    "        \n",
    "        Args:\n",
    "            x_train: Training features (n_samples, n_features)\n",
    "            y_train: Training labels (n_samples,)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.x_train = np.asarray(x_train)\n",
    "        self.y_train = np.asarray(y_train)\n",
    "        self.n_samples = len(self.x_train)\n",
    "        \n",
    "        # Pre-split data into chunks for workers\n",
    "        chunk_size = (self.n_samples + self.n_workers - 1) // self.n_workers\n",
    "        self.data_chunks = []\n",
    "        \n",
    "        for i in range(self.n_workers):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, self.n_samples)\n",
    "            \n",
    "            if start_idx < self.n_samples:\n",
    "                self.data_chunks.append((\n",
    "                    self.x_train[start_idx:end_idx],\n",
    "                    self.y_train[start_idx:end_idx]\n",
    "                ))\n",
    "        \n",
    "        print(f\"Data split into {len(self.data_chunks)} chunks for {self.n_workers} workers\")\n",
    "    \n",
    "    def predire(self, test_sample):\n",
    "        \"\"\"\n",
    "        Predict the class label for a test sample using parallel computation.\n",
    "        \n",
    "        Args:\n",
    "            test_sample: Test sample features (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class label\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'data_chunks'):\n",
    "            raise RuntimeError(\"Model not fitted. Call fitt() first.\")\n",
    "        \n",
    "        test_sample = np.asarray(test_sample)\n",
    "        \n",
    "        # Submit parallel tasks to compute distances for each chunk\n",
    "        futures = []\n",
    "        for x_chunk, y_chunk in self.data_chunks:\n",
    "            future = calculate_distances_chunk(x_chunk, y_chunk, test_sample, self.k)\n",
    "            futures.append(future)\n",
    "        \n",
    "        # Wait for all workers to complete and collect results\n",
    "        chunk_results = [future.result() for future in futures]\n",
    "        \n",
    "        # Merge all k*workers nearest neighbors\n",
    "        all_distances = np.concatenate([result[0] for result in chunk_results])\n",
    "        all_labels = np.concatenate([result[1] for result in chunk_results])\n",
    "        \n",
    "        # Select final k nearest neighbors from merged results\n",
    "        final_k = min(self.k, len(all_distances))\n",
    "        final_k_indices = np.argpartition(all_distances, final_k - 1)[:final_k]\n",
    "        final_k_labels = all_labels[final_k_indices]\n",
    "        \n",
    "        # Return most common label (majority vote)\n",
    "        most_common = Counter(final_k_labels).most_common(1)\n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def predire_batch(self, test_samples):\n",
    "        \"\"\"\n",
    "        Predict labels for multiple test samples.\n",
    "        \n",
    "        Args:\n",
    "            test_samples: Array of test samples (n_test_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            List of predicted labels\n",
    "        \"\"\"\n",
    "        return [self.predire(sample) for sample in test_samples]\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up Parsl resources.\"\"\"\n",
    "        if self.parsl_loaded:\n",
    "            parsl.dfk().cleanup()\n",
    "            parsl.clear()\n",
    "            self.parsl_loaded = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89aed10a-5d7b-4761-a765-f64f3cdc8192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7589ea11bfb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config(\n",
    "                executors=[\n",
    "                    ThreadPoolExecutor(\n",
    "                        max_threads=12,\n",
    "                        label='knn_workers'\n",
    "                    )\n",
    "                ],\n",
    "                strategy='none',  # Disable auto-scaling for predictable performance\n",
    "            )\n",
    "parsl.load(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e4b7a56-98d7-4d46-925f-40be244c953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into 12 chunks for 12 workers\n",
      "Predicted class: [ True]\n",
      "prediction time: 2.04 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "knn_parsl = ParslKNN(k=5, n_workers=12)\n",
    "knn_parsl.fitt(x_train, y_train)\n",
    "\n",
    "debut = time.time()\n",
    "prediction = knn_parsl.predire(x_test)\n",
    "fin = time.time()\n",
    "print(f\"Predicted class: {prediction == y_test}\")\n",
    "print(f'prediction time: {fin-debut:.2f} secondes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d9e6e59-8aa7-459e-89f9-4a69e4755f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsl.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7a8d3-8e1c-4ebf-b62e-35c6c4d106af",
   "metadata": {},
   "source": [
    "ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160d703-74e3-4497-84e3-0a6245886465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e901d9e5-6fba-4e8d-938e-62ea053352da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import ray\n",
    "import time\n",
    "\n",
    "@ray.remote\n",
    "def calculate_distances_chunk(x_chunk, y_chunk, test_sample, k):\n",
    "    \"\"\"\n",
    "    Ray remote function: Calculate distances for a data chunk.\n",
    "    Returns k nearest distances and labels from this chunk.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate squared Euclidean distances (skip sqrt for efficiency)\n",
    "    distances = np.sum((x_chunk - test_sample)**2, axis=1)\n",
    "    \n",
    "    # Get k smallest distances from this chunk\n",
    "    chunk_k = min(k, len(distances))\n",
    "    k_indices = np.argpartition(distances, chunk_k - 1)[:chunk_k]\n",
    "    \n",
    "    # Return distances and corresponding labels\n",
    "    return distances[k_indices], y_chunk[k_indices]\n",
    "\n",
    "class RayKNN:\n",
    "    def __init__(self, k, n_workers=4, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize Ray KNN classifier.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of nearest neighbors\n",
    "            n_workers: Number of parallel workers\n",
    "            verbose: Whether to print timing information\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.n_workers = n_workers\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fitt(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the KNN model with training data.\n",
    "        \n",
    "        Args:\n",
    "            x_train: Training features (n_samples, n_features)\n",
    "            y_train: Training labels (n_samples,)\n",
    "        \"\"\"\n",
    "        self.x_train = np.asarray(x_train)\n",
    "        self.y_train = np.asarray(y_train)\n",
    "        self.n_samples = len(self.x_train)\n",
    "        \n",
    "        # Pre-split data into chunks for workers\n",
    "        chunk_size = (self.n_samples + self.n_workers - 1) // self.n_workers\n",
    "        self.data_chunks = []\n",
    "        \n",
    "        for i in range(self.n_workers):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, self.n_samples)\n",
    "            \n",
    "            if start_idx < self.n_samples:\n",
    "                self.data_chunks.append((\n",
    "                    self.x_train[start_idx:end_idx],\n",
    "                    self.y_train[start_idx:end_idx]\n",
    "                ))\n",
    "        \n",
    "        print(f\"Data split into {len(self.data_chunks)} chunks for {self.n_workers} workers\")\n",
    "    \n",
    "    def predire(self, test_sample):\n",
    "        \"\"\"\n",
    "        Predict the class label for a test sample using parallel computation.\n",
    "        \n",
    "        Args:\n",
    "            test_sample: Test sample features (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class label\n",
    "        \"\"\"\n",
    "        overall_start = time.time()\n",
    "        \n",
    "        if not hasattr(self, 'data_chunks'):\n",
    "            raise RuntimeError(\"Model not fitted. Call fitt() first.\")\n",
    "        \n",
    "        # Convert test sample\n",
    "        convert_start = time.time()\n",
    "        test_sample = np.asarray(test_sample)\n",
    "        convert_time = time.time() - convert_start\n",
    "        \n",
    "        # Submit parallel tasks to compute distances for each chunk\n",
    "        submit_start = time.time()\n",
    "        futures = []\n",
    "        for x_chunk, y_chunk in self.data_chunks:\n",
    "            future = calculate_distances_chunk.remote(x_chunk, y_chunk, test_sample, self.k)\n",
    "            futures.append(future)\n",
    "        submit_time = time.time() - submit_start\n",
    "        \n",
    "        # Wait for all workers to complete and collect results\n",
    "        wait_start = time.time()\n",
    "        chunk_results = ray.get(futures)\n",
    "        wait_time = time.time() - wait_start\n",
    "        \n",
    "        # Merge all k*workers nearest neighbors\n",
    "        merge_start = time.time()\n",
    "        all_distances = np.concatenate([result[0] for result in chunk_results])\n",
    "        all_labels = np.concatenate([result[1] for result in chunk_results])\n",
    "        merge_time = time.time() - merge_start\n",
    "        \n",
    "        # Select final k nearest neighbors from merged results\n",
    "        final_select_start = time.time()\n",
    "        final_k = min(self.k, len(all_distances))\n",
    "        final_k_indices = np.argpartition(all_distances, final_k - 1)[:final_k]\n",
    "        final_k_labels = all_labels[final_k_indices]\n",
    "        final_select_time = time.time() - final_select_start\n",
    "        \n",
    "        # Return most common label (majority vote)\n",
    "        vote_start = time.time()\n",
    "        most_common = Counter(final_k_labels).most_common(1)\n",
    "        vote_time = time.time() - vote_start\n",
    "        \n",
    "        total_time = time.time() - overall_start\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n=== PREDIRE TIMING (single sample) ===\")\n",
    "            print(f\"Test sample conversion: {convert_time:.4f}s\")\n",
    "            print(f\"Task submission: {submit_time:.4f}s\")\n",
    "            print(f\"Worker execution (wait): {wait_time:.4f}s\")\n",
    "            print(f\"Result merging: {merge_time:.4f}s\")\n",
    "            print(f\"Final k selection: {final_select_time:.4f}s\")\n",
    "            print(f\"Majority voting: {vote_time:.4f}s\")\n",
    "            print(f\"Total predire time: {total_time:.4f}s\")\n",
    "        \n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def predire_batch(self, test_samples):\n",
    "        \"\"\"\n",
    "        Predict labels for multiple test samples.\n",
    "        \n",
    "        Args:\n",
    "            test_samples: Array of test samples (n_test_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            List of predicted labels\n",
    "        \"\"\"\n",
    "        return [self.predire(sample) for sample in test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca172a95-8afd-4c05-921c-6147a8ee8a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 11:30:21,852\tINFO worker.py:2013 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 12.0, 'node:__internal_head__': 1.0, 'memory': 15262773658.0, 'object_store_memory': 6541188710.0, 'node:192.168.62.206': 1.0}\n",
      "Data split into 12 chunks for 12 workers\n",
      "\n",
      "=== PREDIRE TIMING (single sample) ===\n",
      "Test sample conversion: 0.0000s\n",
      "Task submission: 2.9503s\n",
      "Worker execution (wait): 1.0277s\n",
      "Result merging: 0.0001s\n",
      "Final k selection: 0.0001s\n",
      "Majority voting: 0.0001s\n",
      "Total predire time: 3.9783s\n",
      "prediction time = 3.9801 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ray.init(num_cpus=12)\n",
    "print(ray.cluster_resources())\n",
    "knn = RayKNN(5, 12)\n",
    "knn.fitt(x_train, y_train)\n",
    "debut = time.time()\n",
    "prediction = knn.predire(x_test)\n",
    "fin = time.time()\n",
    "print(f'prediction time = {fin-debut:.4f} secondes')\n",
    "prediction == y_test\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6831938b-73fe-476c-96bd-c3214dbb229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deabc04-40a2-4d23-bee1-e2d8b5b4c2db",
   "metadata": {},
   "source": [
    "dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6c733d-be6b-4403-a978-690d993b96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "from dask.distributed import Client\n",
    "\n",
    "def calculate_distances_chunk(x_chunk, y_chunk, test_sample, k):\n",
    "    \"\"\"\n",
    "    Dask function: Calculate distances for a data chunk.\n",
    "    Returns k nearest distances and labels from this chunk.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Calculate squared Euclidean distances (skip sqrt for efficiency)\n",
    "    distances = np.sum((x_chunk - test_sample)**2, axis=1)\n",
    "    \n",
    "    # Get k smallest distances from this chunk\n",
    "    chunk_k = min(k, len(distances))\n",
    "    k_indices = np.argpartition(distances, chunk_k - 1)[:chunk_k]\n",
    "    \n",
    "    # Return distances and corresponding labels\n",
    "    return distances[k_indices], y_chunk[k_indices]\n",
    "\n",
    "\n",
    "def calculate_distances_wrapper(chunk_tuple, test_sample, k):\n",
    "    \"\"\"\n",
    "    Wrapper function to unpack chunk tuple for Dask submit.\n",
    "    \"\"\"\n",
    "    x_chunk, y_chunk = chunk_tuple\n",
    "    return calculate_distances_chunk(x_chunk, y_chunk, test_sample, k)\n",
    "\n",
    "\n",
    "class DaskKNN:\n",
    "    def __init__(self, k, client, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize Dask KNN classifier.\n",
    "        \n",
    "        Args:\n",
    "            k: Number of nearest neighbors\n",
    "            client: Dask Client instance (required)\n",
    "            verbose: Whether to print timing information\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.client = client\n",
    "        self.verbose = verbose\n",
    "        self.scattered_chunks = None\n",
    "        \n",
    "    def fitt(self, x_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the KNN model with training data.\n",
    "        \n",
    "        Args:\n",
    "            x_train: Training features (n_samples, n_features)\n",
    "            y_train: Training labels (n_samples,)\n",
    "        \"\"\"\n",
    "        if self.client is None:\n",
    "            raise RuntimeError(\"Client not initialized. Pass a Dask Client to __init__().\")\n",
    "        \n",
    "        self.x_train = np.asarray(x_train)\n",
    "        self.y_train = np.asarray(y_train)\n",
    "        self.n_samples = len(self.x_train)\n",
    "        \n",
    "        # Pre-split data into chunks for workers\n",
    "        n_workers = len(self.client.scheduler_info()['workers'])\n",
    "        chunk_size = (self.n_samples + n_workers - 1) // n_workers\n",
    "        data_chunks = []\n",
    "        \n",
    "        for i in range(n_workers):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min(start_idx + chunk_size, self.n_samples)\n",
    "            \n",
    "            if start_idx < self.n_samples:\n",
    "                data_chunks.append((\n",
    "                    self.x_train[start_idx:end_idx],\n",
    "                    self.y_train[start_idx:end_idx]\n",
    "                ))\n",
    "        \n",
    "        # Scatter data chunks to workers for reuse\n",
    "        self.scattered_chunks = self.client.scatter(data_chunks, broadcast=True)\n",
    "        \n",
    "        print(f\"Data split into {len(data_chunks)} chunks for {n_workers} workers\")\n",
    "    \n",
    "    def predire(self, test_sample):\n",
    "        \"\"\"\n",
    "        Predict the class label for a test sample using parallel computation.\n",
    "        \n",
    "        Args:\n",
    "            test_sample: Test sample features (n_features,)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted class label\n",
    "        \"\"\"\n",
    "        overall_start = time.time()\n",
    "        \n",
    "        if self.scattered_chunks is None:\n",
    "            raise RuntimeError(\"Model not fitted. Call fitt() first.\")\n",
    "        \n",
    "        # Convert test sample\n",
    "        convert_start = time.time()\n",
    "        test_sample = np.asarray(test_sample)\n",
    "        convert_time = time.time() - convert_start\n",
    "        \n",
    "        # Submit parallel tasks to compute distances for each chunk\n",
    "        submit_start = time.time()\n",
    "        futures = []\n",
    "        for chunk_future in self.scattered_chunks:\n",
    "            future = self.client.submit(\n",
    "                calculate_distances_wrapper,\n",
    "                chunk_future,\n",
    "                test_sample,\n",
    "                self.k\n",
    "            )\n",
    "            futures.append(future)\n",
    "        submit_time = time.time() - submit_start\n",
    "        \n",
    "        # Wait for all workers to complete and collect results\n",
    "        wait_start = time.time()\n",
    "        chunk_results = self.client.gather(futures)\n",
    "        wait_time = time.time() - wait_start\n",
    "        \n",
    "        # Merge all k*workers nearest neighbors\n",
    "        merge_start = time.time()\n",
    "        all_distances = np.concatenate([result[0] for result in chunk_results])\n",
    "        all_labels = np.concatenate([result[1] for result in chunk_results])\n",
    "        merge_time = time.time() - merge_start\n",
    "        \n",
    "        # Select final k nearest neighbors from merged results\n",
    "        final_select_start = time.time()\n",
    "        final_k = min(self.k, len(all_distances))\n",
    "        final_k_indices = np.argpartition(all_distances, final_k - 1)[:final_k]\n",
    "        final_k_labels = all_labels[final_k_indices]\n",
    "        final_select_time = time.time() - final_select_start\n",
    "        \n",
    "        # Return most common label (majority vote)\n",
    "        vote_start = time.time()\n",
    "        most_common = Counter(final_k_labels).most_common(1)\n",
    "        vote_time = time.time() - vote_start\n",
    "        \n",
    "        total_time = time.time() - overall_start\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\n=== PREDIRE TIMING (single sample) ===\")\n",
    "            print(f\"Test sample conversion: {convert_time:.4f}s\")\n",
    "            print(f\"Task submission: {submit_time:.4f}s\")\n",
    "            print(f\"Worker execution (wait): {wait_time:.4f}s\")\n",
    "            print(f\"Result merging: {merge_time:.4f}s\")\n",
    "            print(f\"Final k selection: {final_select_time:.4f}s\")\n",
    "            print(f\"Majority voting: {vote_time:.4f}s\")\n",
    "            print(f\"Total predire time: {total_time:.4f}s\")\n",
    "        \n",
    "        return most_common[0][0]\n",
    "    \n",
    "    def predire_batch(self, test_samples):\n",
    "        \"\"\"\n",
    "        Predict labels for multiple test samples.\n",
    "        \n",
    "        Args:\n",
    "            test_samples: Array of test samples (n_test_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            List of predicted labels\n",
    "        \"\"\"\n",
    "        return [self.predire(sample) for sample in test_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b79fe217-7b77-44b9-bad4-cd48ea5b5215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Dask Client...\n",
      "Dask Dashboard: http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "client.close()\n",
    "print(\"Starting Dask Client...\")\n",
    "client = Client(n_workers=2, threads_per_worker=1)\n",
    "print(f\"Dask Dashboard: {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de776a-9a8f-46eb-ae7e-a130c2289a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "knn_dask = DaskKNN(5, client=client)\n",
    "begin = time.time()\n",
    "knn_dask.fitt(x_train, y_train)\n",
    "end = time.time()\n",
    "print(f'scatter time = {end - begin:.4f} secondes')\n",
    "debut = time.time()\n",
    "prediction = knn_dask.predire(x_test)\n",
    "fin = time.time()\n",
    "print(f'prediction time = {fin-debut:.4f} secondes')\n",
    "prediction == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcba03c-98e4-48a5-aa5e-616b584ac0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
